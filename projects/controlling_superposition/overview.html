---
layout: default
---
<article class="container mx-auto px-2 mt2 mb4">
	<header>
		<h1 class="h1 py-4 mt-3">Controlling Superposition</h1>
	  </header>
    <div class="sm-width-full border-top-thin">
    </div>
    <div class="prose mb-4 py-4">
    <h2>Introduction</h2>
	<p>
        In line with the first claim of Zoom In, we consider features, mediated as directions in their latent space, to be the fundamental of expression for neural networks. That is, directions in the latent spaces of neural networks are the units by which neural networks form their representations. I am not convinced by this vision, some speculations I have come from the <a href="https://www.lesswrong.com/posts/eDicGjD9yte6FLSie/interpreting-neural-networks-through-the-polytope-lens">Polytope Lens</a>, however, for the simplicity let us adopt this view.
    </p>
    <p>
        Since neural networks store these representations in finite-dimensional vector spaces, it immediately becomes clear from this view that the number of <em>independent</em> features it can learn is bounded by the dimension of the vector space as independence in this context manifests as orthogonal directions. Orthogonality is necessary so that one feature can fire without interfering with the others. Consequently, if neural networks were utilising independent features then they wouldn't be performing as well as they currently are. Furthermore, we wouldn't be able to extract interpretable features from the neural network using dictionary learning with expansion factors larger than one. If this were the case, then mechanistic interpretability would be a relatively easier task, since one would just have to search for an $n$-dimensional basis of features from which to reconstruct the model's behaviour. Therefore, there must be more to the story, and indeed there is due to the fact that neural networks are not <em>fully</em> linear and the existence of Johnson-Lindenstrauss lemma.
    </p>
    <p>
        To extract independent features all we need are linear operations. Therefore, since neural networks are essentially a sequence of linear operations separated by element-wise non-linearity, it suggests that it is perhaps not essential that neural networks store features independently. Indeed, the non-linearities mean that even if features are not completely orthogonal we can still retrieve relatively cleanly using the non-linearities. Although this leniency may seem insignificant, it is exponentially significant thanks to the Johnson-Lindenstrauss lemma; which says that by allowing some fixed small amount of overlap between the directions, we can store exponentially many of the features.
    </p>
    <p>
        So even though now the features are not stored in completely orthogonal directions, the non-linearities mean that we can still extract them relatively cleanly from the model's representations. Moreover, the model can also store features judiciously to limit the impact of this interference. For instance, features that are anti-correlated should be represented in almost antipodal directions, whereas correlated features can be represented with more aligned directions.
    </p>
    <p>
        Of course, this discussion is just speculating on how the neural network behaves, however, there is significant empirical evidence that this is a large part of the neural network story. This idea is summarised as the superposition hypothesis. Understanding the extent to which the superposition hypothesis is in play for certain models is a fascinating question since it has ramifications for how we reason about the behaviours of these models. Superposition is often used as an explanation for polysemantic neurons, which are neurons which are active on seemingly disjoint concepts. This increases the challenge of mechanistic interpretability which attempts to efficiently explain the behaviour of a neural network through analysing the behaviour of its internal components.
    </p>
    <p>
        If there were a way to control the amount of superposition occurring in a neural network or understand to what extent a neural network is relying on superposition, then this would be beneficial for interpretability.
    </p>
    <p>
        Now although superposition can explain part of the story as to why neural networks can seemingly utilise many more features than they have neurons, it is perhaps not the full story. It is noted that compositionality provides an alternative mechanism for neural networks to represent more features than they have dimensions, and it is <a href="https://transformer-circuits.pub/2023/superposition-composition/index.html">argued</a> that these two notions are at odds with one another.
    </p>
    <p>
        This suggests that the superposition of neural networks exists on a spectrum. In some cases there may be a relatively small set of simple features that are highly expressive, meaning that compositionality can be effectively leveraged. On the other hand, features may be plentiful and sparse, in which case the model would benefit from storing features in superposition. Understanding where along this spectrum a neural network lies can be important from a performance and interpretability perspective. In this work, I try to investigate the superposition portion of the story by exploiting its foundation on the non-linearity. 
    </p>
    <h2>Toy Models</h2>
    <p>
        Let us start with the toy model set up of <a href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation">Toy Models of Superposition</a>. We want to intervene on the non-linearity of these models and explore the subsequent effects on superposition. Thankfully, the ReLU provides an elegant way of doing this. By changing the slope of the negative component from $0$ to $1$, we can go from <em>fully</em> non-linearity to linear, with intermediate slopes providing a varying level of non-linearity. 
    </p>
    <p>
        Doing this we observe that we gradually reduce the extent to which the model is superimposing features. With <em>full</em> non-linearity, each of the five features are represented in superposition, whereas for the linear model only two features are represented.
    </p>
    <img src="/projects/controlling_superposition/images/1.png">
    <p>
        This supports our hypothesis that the non-linearity is crucial for controlling the extent to which neural networks can superimpose features.
    </p>
    <p>
        In the second row of the figure we observe the effects this has on the partitioning of the latent space of the neural networks. We observe how the partitioning correspond to the direction of the features. Since, the partitions are inherently related to the non-linearities of the model, it is clear that the non-linearities play a role in the arrangement of the features. 
    </p>

    <h3>Extended</h3>
    <p>
        Here we take the toy model's further and explore its performance in relation to its linearity, the distribution on feature importance and the size of its latent dimension. More specifically, we train the model on five features comprising two semantical concepts, say shape and colour. Namely, we have three shape features, say red, green and blue, and two colour features, say circle and square; so that firing the first shape feature and second colour feature corresponds to a red square. We train the model to reconstruct these various combinations of features, where the propensity of each feature is related to some probability; in our case we'll have the first feature being the most important and the last feature being the least important to some degree for each concept. More specifically, feature <em>i</em> of <em>n</em> for a concept has importance $$p_i=\frac{\exp(-\alpha\gamma i)}{\sum_{j=1}^n\exp(-\alpha\gamma)}$$where $\alpha$$ is a concept level parameter and $\gamma\in[0,1]$ is a skewness parameter, such that for $\gamma=0$ the distribution over the features is uniform and for $\gamma=1$ it is skewed toward the first feature.
    </p>
    <p>
        In training the model only ever sees one-hot vectors combining features from the concepts; whereas in testing we will provide the model with one-hot vectors isolating a feature from one of the concepts.
    </p>
    <p>
        Using the intuition from our previous experiments, we test the following aspects of the trained model on these isolated feature queries.
    </p>
    <ul>
      <li>Reconstruction ability. The mean squared error of the input and output.</li>
      <li>The training loss of the model.</li>
      <li>The minimum and maximum interference between the latent activations of the model on each feature query.</li>
      <li>The interference between the unnormalized and normalized features of the model.</li>
      <li>The magnitude of the features.</li>
    </ul>
    <p>
        We conduct four sets of experiments.
    </p>
    <ol>
      <li>We vary the skewness of the features and linearity of the model when it has two latent dimensions.</li>
      <li>We vary the skewness of the features and the linearity of the model when it has three latent dimensions.</li>
      <li>We vary the dimension of the latent space and linearity of the model when the skewness parameter is set to $0$.</li>
      <li>We vary the dimension of the latent space and linearity of the model when the skewness parameter is set to $0.25$.</li>
    </ol>
    
    <table>
      <thead>
        <tr>
          <th>Metric</th>
          <th>Experiment One</th>
          <th>Experiment Two</th>
          <th>Conclusion</th>
          <th>Supporting Figure</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Reconstruction Error</td>
          <td>Lowest for uniform feature importance and non-linear activation functions.</td>
          <td>Relatively lower, and more evenly distributed across skewness levels. Clear distinction between the linear and non-linear cases</td>
          <td>The higher errors for higher skewness is reflective of the fact that the model puts less emphasis on the lower importance features.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/output_discrepancies_exp2.png" alt="Experiment One" width="200"></td>
        </tr>
        <tr>
          <td>Training Loss</td>
          <td>Highest when the distribution is uniform and lowest when the distribution is skewed. Fairly consistent across all constant linearity levels.</td>
          <td>Fairly consistent across all configurations.</td>
          <td>Demonstrates that the model can adapt to different activation functions. Moreover, constraints seem to dissipate when we increase the dimensionality of the latent space.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/losses_exp1.png" alt="Experiment One" width="200"></td>
        </tr>
        <tr>
          <td>Latent Interference (Minimum)</td>
          <td>Clear separation from the non-linear regime, and increasing as linearity is introduced.</td>
          <td>Same pattern, just slightly lower values.</td>
          <td>The non-linear regimes allows the models to have these negative interferences without affecting performance. As the dimension increases the model can store the features with less interference.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/latent_min_interferences_exp1.png" alt="Experiment One" width="200"></td>
        </tr>
        <tr>
          <td>Latent Interference (Maximum)</td>
          <td>Consistent across all settings.</td>
          <td>Consistent across all settings.</td>
          <td></td>
          <td><img src="/projects/controlling_superposition/images/experiments/latent_max_interferences_exp2.png" alt="Experiment Two" width="200"></td>
        </tr>
        <tr>
          <td>Feature Interference (Unnormalized)</td>
          <td>Consistently low and decreases as linearity increases.</td>
          <td>Match the values of the normalized feature interference.</td>
          <td>Feature can collapse when there is insufficient space to accommodate them. Linearity affects the extent to which features can be represented.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/intra_feature_unnormalised_exp1.png" alt="Experiment One" width="200"></td>
        </tr>
        <tr>
          <td>Feature Interference (Normalized)</td>
          <td>Consistently high.</td>
          <td>There is a clear separation between the uniform and skewed distributions. Interference is lower for uniform distributions as the features are likely to co-occur and thus must be separated to limit interference.</td>
          <td>The model actively distributes features according to the importance of the features, and not necessarily impacted by the linearity of the model.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/intra_feature_normalised_exp2.png" alt="Experiment Two" width="200"></td>
        </tr>
        <tr>
          <td>Feature Magnitude</td>
          <td>Insignificant features tend to collapse.</td>
          <td>Insignificant features do not collapse.</td>
          <td>When the model has space, it will continue to represent even insignificant features.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/feature_magnitude_exp2.png" alt="Experiment Two" width="200"></td>
        </tr>
      </tbody>
    </table>
    
    <table>
      <thead>
        <tr>
          <th>Metric</th>
          <th>Experiment Three</th>
          <th>Experiment Four</th>
          <th>Conclusion</th>
          <th>Supporting Figure</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Reconstruction Error</td>
          <td>Highest for the low-dimensional models and highly linear models.</td>
          <td>Same pattern but more abrupt.</td>
          <td>Due to the skewness the model is focusing its attention more on certain features meaning the errors change more dramatically.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/output_discrepancies_exp3.png" alt="Experiment Three" width="200"></td>
        </tr>
        <tr>
          <td>Training Loss</td>
          <td>Constant across linearities. However, there is a clear distinction between the two-dimensional models and the other models.</td>
          <td>Relatively lower values but pattern is the same.</td>
          <td>There is something significant about two-dimensions that means it is severely limited in representing five features compared to higher-dimensional models.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/losses_exp3.png" alt="Experiment Three" width="200"></td>
        </tr>
        <tr>
          <td>Latent Interference (Minimum)</td>
          <td>Highest for low-dimensional and non-linear models.</td>
          <td>Same pattern.</td>
          <td>Reflective of how the non-linearity allows the model to represent features with greater negative interference. Higher-dimensional models do not require this as they have more space to store features.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/latent_min_interferences_exp4.png" alt="Experiment Four" width="200"></td>
        </tr>
        <tr>
          <td>Latent Interference (Maximum)</td>
          <td>Consistent across all settings.</td>
          <td>Consistent across all settings.</td>
          <td></td>
          <td><img src="/projects/controlling_superposition/images/experiments/latent_max_interferences_exp4.png" alt="Experiment Four" width="200"></td>
        </tr>
        <tr>
          <td>Feature Interference (Unnormalized)</td>
          <td>Remaining fairly constant across settings, with three-dimensional models seeing a strangely low amount of interference.</td>
          <td>Remaining fairly constant across settings, with three-dimensional models seeing a strangely low amount of interference.</td>
          <td></td>
          <td><img src="/projects/controlling_superposition/images/experiments/intra_feature_unnormalised_exp3.png" alt="Experiment Three" width="200"></td>
        </tr>
        <tr>
          <td>Feature Interference (Normalized)</td>
          <td>High for two-dimensional models and then dramatically drops.</td>
          <td>High for two-dimensional models and then dramatically drops.</td>
          <td>Reflective of how the model is severely limited in the two-dimensional regimes, and the models are always trying to separate out the features.</td>
          <td><img src="/projects/controlling_superposition/images/experiments/intra_feature_normalised_exp3.png" alt="Experiment Three" width="200"></td>
        </tr>
        <tr>
          <td>Feature Magnitude</td>
          <td>Relatively mild feature collapse, with any signs of feature collapse disappearing as latent dimensions increases.</td>
          <td>Pattern is more pronounced.</td>
          <td></td>
          <td><img src="/projects/controlling_superposition/images/experiments/feature_magnitudes_exp4.png" alt="Experiment Four" width="200"></td>
        </tr>
      </tbody>
    </table>
    




    <h2>InceptionV1</h2>
    <h3>Layer Intervention</h3>
    <p>
        We now extrapolate these ideas beyond the toy model and to InceptionV1. More specifically, we try to understand the extent to which this model superimposes features by intervening on each of the blocks of the model. For each block we modify the slope of the negative component of its ReLU activations, and then test the performance of the model on a subset of the Imagenet-1K dataset. The intention is that if a block heavily relies on superposition, then intervening on it in this way will more negatively impact its performance.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies.png">
    <p>
        Interestingly what we see is that the inception3a and inception4e blocks are the most effected by these interventions, whereas the inception5b block is least effected.
    </p>
    <ul>
        <li>
            The inception3a block has the fewest number of neurons, so it would make sense that it holds more features in superposition. Moreover, the block is in the early part of the model, which has been identified to capture more textual features. Since textual features are usually more sparse than compositional, it make sense that the model utilises superposition here. 
        </li>
        <li>
            The inception4e block actually has a relatively large capacity, however, in other analyses it has been identified to have polysemantic neurons, suggesting that it is also reliant on superposition. 
        </li>
        <li>
            The inception5b block is the last block in the model and has $1024$ neurons, which is more than the $1000$ classes of Imagenet-1K. Therefore, since features are the most developed at this stage we would expect that they largely correspond to the individual classes, and thus the model does not need superposition as it can afford to assign features to orthogonal directions. 
        </li>
    </ul>
    <h3>Prediction Collapse</h3>
    <p>
        For a model that stores features in superposition, removing the non-linearities would limit the model's capacity to retrieve those features, and would introduce interference between them. In particular, what we observe is that the model's predictions tend to collapse onto a few classes. Potentially due to the fact that the interference at the intervened layer causes the activations of neurons to drop into negative territory such that at subsequent layers they are thresholded and no longer contribute to the network's performance. Understanding how the predictions of classes of images collapse under these interventions can perhaps shed light into how features are distributed across neurons.
    </p>
    <img src="/projects/controlling_superposition/images/class_prediction_frequency.png">
    <p>
        In this figure we give each class a colour, and plot a rectangle of height equal to the frequency with which it appears as a prediction across as sample of Imagenet-1K of size $4096$. On the horizontal axis we have the slope of the intervention. The dashed black line gives the number of neurons in the layer, in this case we are looking at the inception4e block which has $832$ neurons, and the solid red line indicates the number of distinct class predictions made across the dataset. Clearly what we see is that the number of distinct predictions given by the model dramatically decreases, leaving only a few classes being regularly predicted. In this case they happened to be "buckle" which was predicted $1093$ times, "mask" which was predicted $326$ times, "chest" which was predicted $171$ times amongst others. Therefore, we can conclude that features corresponding to these class are prominent across the neurons of this block.
    </p>
    <p>
        This idea that the non-linearity helps retrieve the features in non-trivial ways to produce their predictions is reinforced by observing the confidence with which the model makes its predictions at varying levels of intervention.
    </p>
    <p>
        With no intervention the model expresses a wide-ranges of confidences due to the fact that some classes are very similar and so its cannot classify them certainty.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_00.png">
    <p>
        As we start to increase the slope of the negative component we see that the confidence with which the model makes its prediction gets gradually more skewed.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_00693.png">
    <p>
        The above figure is with a negative slope of $0.0693$. When we then consider a fully linear layer we observe that the predictions are very much skewed to being very confident.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_10.png">
    <p>
        This is reflective of the fact that the model has fewer linearities to extract the nuance in the features. These figures were obtained from our intervention n the inception5b block.
    </p>
    <h3>Neuron Intervention</h3>
    <p>
        We can take this analysis further by intervening on specific neurons of the network. From <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a>, it was found that the 55th neuron from the inception4e block exhibited some polysemanticity; potentially because it is retrieving concepts from superposition. When intervening on this neuron specifically, we observe significant changes in the confidence with which the model makes its predictions.
    </p>
    <img src="/projects/controlling_superposition/images/class_confidences_4e55.png">
    <p>
        The above figure depicts the change in the average confidence given by the model to inputs labelled with the corresponding class as we intervene on the neuron; with the left plot showing the classes that experiences the largest drop in confidence and the right plot showing the classes that experiences the largest increase in confidence. Below is the same results for a random baseline, which is where we just perform the intervention on the first neuron of the inception3a block.
    </p>
    <img src="/projects/controlling_superposition/images/class_confidences_3a1.png">
    <p>
        Clearly, the changes in this instance are not as significant.
    </p>
    <h2>From Scratch</h2>
    <p>
        In the previous section we considered a model trained with ReLU activations and then explored its dependency on it by intervening on the layers. Here we instead train models with different activation functions from scratch.
    </p>
    <p>
        We consider an autoencoder trained to reconstruct MNIST images, and a fully connected network to classify MNIST images as our testing grounds for ideas. In these settings, we can explore the necessity and rate of superposition by modifying various model parameters. 
    </p>
    <h3>Autoencoder</h3>
    <p>
        Here our model architecture consists of an encoder and decoder component, and we test the effect of interventions when we train the model with different levels of linearity at the intersection of these blocks. We measure performance by the mean squared reconstruction error.
    </p>
    <h4>The Effect of Latent Dimension</h4>
    <p>
        For a latent dimension $d$, the encoder architecture is $[784,128,64,d]$ and our decoder architecture is $[d,64,128,784]$. At the intersection of these blocks we apply an activation function that is linear for positive values and for negative values it has a slope in the range $\{0.0,0.1,0.4,0.6,0.9,1.0\}$. We consider latent dimensions $d\in\{8,16,32,64\}$ and train the models for 10 epochs on the MNIST train set on each of these different slopes for the activation function. After training we then intervene on the activation functions, by varying their slopes and observing the subsequent reconstruction loss.
    </p>
    <img src="/projects/controlling_superposition/images/fixed_latent_dim.png">
    <p>
        The rows of the figure correspond to architectures of varying latent dimension size. The left figures are depict the reconstruction loss as we intervene on the activation functions, the middle figures are the same plot just with the y-axis capped at 2, and the right figures depict the normalised reconstruction loss. The black marker points on each line identify the slope with which the instance we trained with, which also corresponds the colour of the lines. 
    </p>
    <p>
        From this we make the following observations.
    </p>
    <ul>
        <li>
            We observe that the reconstruction error can vary significantly as we intervene on the activation functions, suggesting that the model is relying on the non-linearity and representing features in superposition. Since the larger models appear to have a relatively flatter curve around the black marker, we could conclude that the larger models are less reliant on the non-linearity and hence are not utilising superposition as much.
        </li>
        <li>
            Each instance of the model, are able to achieve relatively low reconstruction losses. In particular, the reconstruction losses appear to be fairly constant across every slope. Indicating that models can adapt their representations based on the amount of non-linearity they posses. Suggesting that superposition may be a sufficient but not necessary for model performance. Furthermore, the right most figures indicate that the more linear models are robust to subsequent interventions. 
        </li>
        <li>
            As the model gets larger, the reconstruction loss introduced upon intervention gets smaller. Indicating that larger models are perhaps less reliant on the non-linearities for their performance, and can better distribute their representations, because the introduction of non-linearity does not cancel out interference the model is reliant on to form its constructions.
        </li>
        <li>
            The performance of the fully-linear models is relatively consistent across model sizes. Perhaps indicating that the encoder block is only able to extract a few linear features to represent in the latent space. If there were the case, we would expect that increasing the size of the encoder block may be able to yield a lower reconstruction error in the linear setting, we'll explore this in the next section.
        </li>
    </ul>
    <p>
        From these observations we conclude that the non-linearity significantly affects the type of representation learned by the model.
    </p>
    <h4>The Effect of Encoder Size</h4>
    <p>
        Here, we fix the latent dimension of the autoencoder to $d=32$ and varying the layer sizes of the encoder block with $e=\{e_1,e_2\}\in\mathbb{N}^2$, so that the encoder block has architecture [784,$e_1$,$e_2$,32]. Training this model proceeds as before, however, we only consider the fully non-linear and fully linear regimes. We then test the reconstruction losses of these models using the same activation function that they were trained on.
    </p>
    <img src="/projects/controlling_superposition/images/nonlinear_linear_comparison.png">
    <p>
        From the above figure we observe that as we increase the size of the encoder block the loss of the linear models decreases, as expected since now the encoder block is able to supply the latent space with linear features to represent. We observe that increase the encoder size beyond a certain size doesn't yield better performance, on the contrary, performance starts to degrade. For these larger encoder sizes, it may be the case that they are too powerful, and providing the latent space too many linear features to represent. In this set up, the latent space can only store a maximum of $32$ features not in superposition.
    </p>
    <h3>Fully Connected</h3>
    <h4>MNIST</h4>
    <p>
        We train a feed-forward neural network on the MNIST classification. We utilise an architecture of the form [784,16,10], which we train with activation functions whose slope for the negative component varies. Once we have this trained model we re-instantiate it with activation functions with different negative slopes and observe the consequences on performance. The slopes for the negative component which we consider are $\{0.0,0.05,0.15,0.25,0.4,0.6,0.75,0.9\}$. We obtain the following results.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_from_scratch.png">
    <p>
        In the figure, each line corresponds to the accuracy of the model that was trained with activation functions whose slope of the negative component is given by the slope identified at the black scatter point.
    </p>
    <p>
        What we observe is that generally this task is not too reliant on superposition, since accuracy does not vary too much on the interventions. In comparison to the reconstruction setting, where the loss varied significantly as we intervened on the activation functions. Suggesting that for this task the model is representing features compositionally rather than in superposition.
    </p>
    <p>
        However, we do observe that training with some slope to the negative component improves the robustness of the model to these interventions, suggesting that it promotes less superposition in the model. This would suggest that to arrive at more interpretable models we should consider training with less <emph>non-linear</emph> activation functions.
    </p>
    <p>
        If we were now to consider a neural network with architecture [784,8,10], we would expect their to be more superposition. Hence, we should observe that the effects of intervening on the activation functions is more significant.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_from_scratch_8.png">
    <h4>Topological Task</h4>
    <p>
        The superposition hypothesis relies on the assumption that the latent representation of neural networks has some <emph>linearity</emph>. In this case, features can be represented as directions and the purpose of the non-linearity is to extract those features to perform predictions. In our previous examples we have been intervening on layers where it is reasonable to assume that the representations have some linearity. For example, in the MNIST example even though our network is dealing directly with the 784-dimensional image manifold, since it is very high-dimensional it is likely to exhibit some linear structure. In these cases, we intuited that the non-linearity allowed the neural network to store features in superposition, and thus we could control or identify the rate of superposition by intervening on the amount of non-linearity these function possessed.  
    </p>
    <p>
        However, now we want to explore what happens to our framework when this linearity in the latent representations is not present. For that we consider the task of classifying a sphere within a large sphere with a hollow centre.
    </p>
    <img src="/projects/controlling_superposition/images/topological_problem.png">
    <p>
        We repeat the procedure as in the case of MNIST, and we obtain the following results when considering a feed-forward neural network of architecture [3,8,2].
    </p>
    <p>
        Where before we saw that performance translated relatively well across slopes, here we observe that it does not translate as well. Suggesting that the non-linearity in this instance is performing more complex operations the do not transfer well to activation functions with different amounts of non-linearity.
    </p>
    <p>
        If instead we first pass through the input data through a layer with a ReLU non-linearity, we improve the linearity of the representations and the interventions exhibit more predictable behaviour.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_topological_linear.png">
    <h2>Conclusion</h2>
    <p>
        Throughout we have investigated how the amount of <emph>non-linearity</emph> a neural network has effects it performance. In particular, we used this to study the notion of superposition, which is a hypothesis on neural network behaviour to explain why it can represent many more features than it has neurons. We found in toy-models that the amount of non-linearity effects the rate of superposition. We then extrapolated these ideas to explore the InceptionV1 models, and we observed how changing the non-linearity of a pre-trained model can cause its representations to collapse leading to a reduction in performance. We then focused on training models a priori with different amounts of non-linearity to observe how its performance would change when we intervened on it by changing its non-linearity. For that autoencoder models for reconstruction potentially rely more on superposition, whereas, fully connected models for classification perhaps rely more on compositionality. Overall, we found that the activation function used significantly influenced the type of representation the model learned.
    </p>
    <p>
        Reducing the amount of superposition in models is desired to improve our ability to perform interpretability on the model. Our observation that we can potentially achieve this, whilst maintaining performance, by reducing the amount of initial non-linearity in the model may prove useful to developing more interpretable models. Moreover, our techniques can be used to establish the amount of superposition in a pre-trained model. This knowledge would be useful when implementing interpretability techniques. 
    </p>
    <p>
        In future work we would like to explore this idea further, by exploring these interventions in more scenarios, and performing interpretability analyses on these models.
    </p>
    <p>
        <a href="https://github.com/ThomasWalker1/expositions/tree/main/controlling_superposition">code</a>
    </p>
    </div>
</article>