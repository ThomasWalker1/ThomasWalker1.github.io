---
layout: default
---
<article class="container mx-auto px-2 mt2 mb4">
	<header>
		<h1 class="h1 py-4 mt-3">Controlling Superposition</h1>
	  </header>
    <div class="sm-width-full border-top-thin">
    </div>
    <div class="prose mb-4 py-4">
    <h2>Introduction</h2>
	<p>
        In line with the first claim of Zoom In, we consider features, mediated as directions in their latent space, to be the fundamental of expression for neural networks. That is, directions in the latent spaces of neural networks are the units by which neural networks form their representations. I am not convinced by this vision, some speculations I have come from the <a href="https://www.lesswrong.com/posts/eDicGjD9yte6FLSie/interpreting-neural-networks-through-the-polytope-lens">Polytope Lens</a>, however, for the simplicity let us adopt this view.
    </p>
    <p>
        Since neural networks store these representations in finite-dimensional vector spaces, it immediately becomes clear from this view that the number of <em>independent</em> features it can learn is bounded by the dimension of the vector space as independence in this context manifests as orthogonal directions. Orthogonality is necessary so that one feature can fire without interfering with the others. Consequently, if neural networks were utilising independent features then they wouldn't be performing as well as they currently are. Furthermore, we wouldn't be able to extract interpretable features from the neural network using dictionary learning with expansion factors larger than one. If this were the case, then mechanistic interpretability would be a relatively easier task, since one would just have to search for an $n$-dimensional basis of features from which to reconstruct the model's behaviour. Therefore, there must be more to the story, and indeed there is due to the fact that neural networks are not <em>fully</em> linear and the existence of Johnson-Lindenstrauss lemma.
    </p>
    <p>
        To extract independent features all we need are linear operations. Therefore, since neural networks are essentially a sequence of linear operations separated by element-wise non-linearity, it suggests that it is perhaps not essential that neural networks store features independently. Indeed, the non-linearities mean that even if features are not completely orthogonal we can still retrieve relatively cleanly using the non-linearities. Although this leniency may seem insignificant, it is exponentially significant thanks to the Johnson-Lindenstrauss lemma; which says that by allowing some fixed small amount of overlap between the directions, we can store exponentially many of the features.
    </p>
    <p>
        So even though now the features are not stored in completely orthogonal directions, the non-linearities mean that we can still extract them relatively cleanly from the model's representations. Moreover, the model can also store features judiciously to limit the impact of this interference. For instance, features that are anti-correlated should be represented in almost antipodal directions, whereas correlated features can be represented with more aligned directions.
    </p>
    <p>
        Of course, this discussion is just speculating on how the neural network behaves, however, there is significant empirical evidence that this is a large part of the neural network story. This idea is summarised as the superposition hypothesis. Understanding the extent to which the superposition hypothesis is in play for certain models is a fascinating question since it has ramifications for how we reason about the behaviours of these models. Superposition is often used as an explanation for polysemantic neurons, which are neurons which are active on seemingly disjoint concepts. This increases the challenge of mechanistic interpretability which attempts to efficiently explain the behaviour of a neural network through analysing the behaviour of its internal components.
    </p>
    <p>
        If there were a way to control the amount of superposition occurring in a neural network or understand to what extent a neural network is relying on superposition, then this would be beneficial for interpretability.
    </p>
    <p>
        Now although superposition can explain part of the story as to why neural networks can seemingly utilise many more features than they have neurons, it is perhaps not the full story. It is noted that compositionality provides an alternative mechanism for neural networks to represent more features than they have dimensions, and it is <a href="https://transformer-circuits.pub/2023/superposition-composition/index.html">argued</a> that these two notions are at odds with one another.
    </p>
    <p>
        This suggests that the superposition of neural networks exists on a spectrum. In some cases there may be a relatively small set of simple features that are highly expressive, meaning that compositionality can be effectively leveraged. On the other hand, features may be plentiful and sparse, in which case the model would benefit from storing features in superposition. Understanding where along this spectrum a neural network lies can be important from a performance and interpretability perspective. In this work, I try to investigate the superposition portion of the story by exploiting its foundation on the non-linearity. 
    </p>
    <h2>Toy Models</h2>
    <p>
        Let us start with the toy model set up of <a href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation">Toy Models of Superposition</a>. We want to intervene on the non-linearity of these models and explore the subsequent effects on superposition. Thankfully, the ReLU provides an elegant way of doing this. By changing the slope of the negative component from $0$ to $1$, we can go from <em>fully</em> non-linearity to linear, with intermediate slopes providing a varying level of non-linearity. 
    </p>
    <p>
        Doing this we observe that we gradually reduce the extent to which the model is superimposing features. With <em>full</em> non-linearity, each of the five features are represented in superposition, whereas for the linear model only two features are represented.
    </p>
    <img src="/projects/controlling_superposition/images/1.png">
    <p>
        This supports our hypothesis that the non-linearity is crucial for controlling the extent to which neural networks can superimpose features.
    </p>
    <p>
        In the second row of the figure we observe the effects this has on the partitioning of the latent space of the neural networks. We observe how the partitioning correspond to the direction of the features. Since, the partitions are inherently related to the non-linearities of the model, it is clear that the non-linearities play a role in the arrangement of the features. 
    </p>
    <h2>InceptionV1</h2>
    <h3>Layer Intervention</h3>
    <p>
        We now extrapolate these ideas beyond the toy model and to InceptionV1. More specifically, we try to understand the extent to which this model superimposes features by intervening on each of the blocks of the model. For each block we modify the slope of the negative component of its ReLU activations, and then test the performance of the model on a subset of the Imagenet-1K dataset. The intention is that if a block heavily relies on superposition, then intervening on it in this way will more negatively impact its performance.
    </p>
    <img src="/projects/controlling_superposition/accuracies.png">
    <p>
        Interestingly what we see is that the inception3a and inception4e blocks are the most effected by these interventions, whereas the inception5b block is least effected.
    </p>
    <ul>
        <li>
            The inception3a block has the fewest number of neurons, so it would make sense that it holds more features in superposition. Moreover, the block is in the early part of the model, which has been identified to capture more textual features. Since textual features are usually more sparse than compositional, it make sense that the model utilises superposition here. 
        </li>
        <li>
            The inception4e block actually has a relatively large capacity, however, in other analyses it has been identified to have polysemantic neurons, suggesting that it is also reliant on superposition. 
        </li>
        <li>
            The inception5b block is the last block in the model and has $1024$ neurons, which is more than the $1000$ classes of Imagenet-1K. Therefore, since features are the most developed at this stage we would expect that they largely correspond to the individual classes, and thus the model does not need superposition as it can afford to assign features to orthogonal directions. 
        </li>
    </ul>
    <h3>Prediction Collapse</h3>
    <p>
        For a model that stores features in superposition, removing the non-linearities would limit the model's capacity to retrieve those features, and would introduce interference between them. In particular, what we observe is that the model's predictions tend to collapse onto a few classes. Potentially due to the fact that the interference at the intervened layer causes the activations of neurons to drop into negative territory such that at subsequent layers they are thresholded and no longer contribute to the network's performance. Understanding how the predictions of classes of images collapse under these interventions can perhaps shed light into how features are distributed across neurons.
    </p>
    <img src="/projects/controlling_superposition/images/class_prediction_frequency.png">
    <p>
        In this figure we give each class a colour, and plot a rectangle of height equal to the frequency with which it appears as a prediction across as sample of Imagenet-1K of size $4096$. On the horizontal axis we have the slope of the intervention. The dashed black line gives the number of neurons in the layer, in this case we are looking at the inception4e block which has $832$ neurons, and the solid red line indicates the number of distinct class predictions made across the dataset. Clearly what we see is that the number of distinct predictions given by the model dramatically decreases, leaving only a few classes being regularly predicted. In this case they happened to be "buckle" which was predicted $1093$ times, "mask" which was predicted $326$ times, "chest" which was predicted $171$ times amongst others. Therefore, we can conclude that features corresponding to these class are prominent across the neurons of this block.
    </p>
    <p>
        This idea that the non-linearity helps retrieve the features in non-trivial ways to produce their predictions is reinforced by observing the confidence with which the model makes its predictions at varying levels of intervention.
    </p>
    <p>
        With no intervention the model expresses a wide-ranges of confidences due to the fact that some classes are very similar and so its cannot classify them certainty.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_00.png">
    <p>
        As we start to increase the slope of the negative component we see that the confidence with which the model makes its prediction gets gradually more skewed.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_00693.png">
    <p>
        The above figure is with a negative slope of $0.0693$. When we then consider a fully linear layer we observe that the predictions are very much skewed to being very confident.
    </p>
    <img src="/projects/controlling_superposition/images/confidences_10.png">
    <p>
        This is reflective of the fact that the model has fewer linearities to extract the nuance in the features. These figures were obtained from our intervention n the inception5b block.
    </p>
    <h3>Neuron Intervention</h3>
    <p>
        We can take this analysis further by intervening on specific neurons of the network. From <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a>, it was found that the 55th neuron from the inception4e block exhibited some polysemanticity; potentially because it is retrieving concepts from superposition. When intervening on this neuron specifically, we observe significant changes in the confidence with which the model makes its predictions.
    </p>
    <img src="/projects/controlling_superposition/images/class_confidences_4e55.png">
    <p>
        The above figure depicts the change in the average confidence given by the model to inputs labelled with the corresponding class as we intervene on the neuron; with the left plot showing the classes that experiences the largest drop in confidence and the right plot showing the classes that experiences the largest increase in confidence. Below is the same results for a random baseline, which is where we just perform the intervention on the first neuron of the inception3a block.
    </p>
    <img src="/projects/controlling_superposition/images/class_confidences_3a1.png">
    <p>
        Clearly, the changes in this instance are not as significant.
    </p>
    <h2>From Scratch</h2>
    <p>
        In the previous section we considered a model trained with ReLU activations and then explored its dependency on it by intervening on the layers. Here we instead train models with different activation functions from scratch.
    </p>
    <p>
        We consider an autoencoder trained to reconstruct MNIST images, and a fully connected network to classify MNIST images as our testing grounds for ideas. In these settings, we can explore the necessity and rate of superposition by modifying various model parameters. 
    </p>
    <h3>Autoencoder</h3>

    <h3>Fully Connected</h3>
    <h4>MNIST</h4>
    <p>
        We train a feed-forward neural network on the MNIST classification. We utilise an architecture of the form [784,16,10], which we train with activation functions whose slope for the negative component varies. Once we have this trained model we re-instantiate it with activation functions with different negative slopes and observe the consequences on performance. The slopes for the negative component which we consider are $\{0.0,0.05,0.15,0.25,0.4,0.6,0.75,0.9\}$. We obtain the following results.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_from_scratch.png">
    <p>
        In the figure, each line corresponds to the accuracy of the model that was trained with activation functions whose slope of the negative component is given by the slope identified at the black scatter point.
    </p>
    <p>
        What we observe is that generally this task is not too reliant on superposition, since accuracy does not vary too much on the interventions. In comparison to the reconstruction setting, where the loss varied significantly as we intervened on the activation functions. Suggesting that for this task the model is representing features compositionally rather than in superposition.
    </p>
    <p>
        However, we do observe that training with some slope to the negative component improves the robustness of the model to these interventions, suggesting that it promotes less superposition in the model. This would suggest that to arrive at more interpretable models we should consider training with less *non-linear* activation functions.
    </p>
    <p>
        If we were now to consider a neural network with architecture [784,8,10], we would expect their to be more superposition. Hence, we should observe that the effects of intervening on the activation functions is more significant.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_from_scratch_8.png">
    <h4>Topological Task</h4>
    <p>
        The superposition hypothesis relies on the assumption that the latent representation of neural networks has some *linearity*. In this case, features can be represented as directions and the purpose of the non-linearity is to extract those features to perform predictions. In our previous examples we have been intervening on layers where it is reasonable to assume that the representations have some linearity. For example, in the MNIST example even though our network is dealing directly with the 784-dimensional image manifold, since it is very high-dimensional it is likely to exhibit some linear structure. In these cases, we intuited that the non-linearity allowed the neural network to store features in superposition, and thus we could control or identify the rate of superposition by intervening on the amount of non-linearity these function possessed.  
    </p>
    <p>
        However, now we want to explore what happens to our framework when this linearity in the latent representations is not present. For that we consider the task of classifying a sphere within a large sphere with a hollow centre.
    </p>
    <img src="/projects/controlling_superposition/images/topological_problem.png">
    <p>
        We repeat the procedure as in the case of MNIST, and we obtain the following results when considering a feed-forward neural network of architecture [3,8,2].
    </p>
    <p>
        Where before we saw that performance translated relatively well across slopes, here we observe that it does not translate as well. Suggesting that the non-linearity in this instance is performing more complex operations the do not transfer well to activation functions with different amounts of non-linearity.
    </p>
    <p>
        If instead we first pass through the input data through a layer with a ReLU non-linearity, we improve the linearity of the representations and the interventions exhibit more predictable behaviour.
    </p>
    <img src="/projects/controlling_superposition/images/accuracies_topological_linear.png">
    <h2>Conclusion</h2>
    <p>
        Throughout we have investigated how the amount of *non-linearity* a neural network has effects it performance. In particular, we used this to study the notion of superposition, which is a hypothesis on neural network behaviour to explain why it can represent many more features than it has neurons. We found in toy-models that the amount of non-linearity effects the rate of superposition. We then extrapolated these ideas to explore the InceptionV1 models, and we observed how changing the non-linearity of a pre-trained model can cause its representations to collapse leading to a reduction in performance. We then focused on training models a priori with different amounts of non-linearity to observe how its performance would change when we intervened on it by changing its non-linearity. For that autoencoder models for reconstruction potentially rely more on superposition, whereas, fully connected models for classification perhaps rely more on compositionality. Overall, we found that the activation function used significantly influenced the type of representation the model learned.
    </p>
    <p>
        Reducing the amount of superposition in models is desired to improve our ability to perform interpretability on the model. Our observation that we can potentially achieve this, whilst maintaining performance, by reducing the amount of initial non-linearity in the model may prove useful to developing more interpretable models. Moreover, our techniques can be used to establish the amount of superposition in a pre-trained model. This knowledge would be useful when implementing interpretability techniques. 
    </p>
    <p>
        In future work we would like to explore this idea further, by exploring these interventions in more scenarios, and performing interpretability analyses on these models.
    </p>
    <p>
        <a href="https://github.com/ThomasWalker1/expositions/tree/main/controlling_superposition">code</a>
    </p>
    </div>
</article>