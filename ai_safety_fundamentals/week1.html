<!DOCTYPE html>
<html>
<head>
<title>Artificial General Intelligence</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<script>
	MathJax = {
	  tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  svg: {
		fontCache: 'global'
	  }
	};
	</script>
<script type="text/javascript" id="MathJax-script" async
	src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="week1-consolidation">Consolidation of Week 1 Reading</h1>
<h2>Four Background Claims (Nate Soares)</h2>
<a href="https://intelligence.org/2015/07/24/four-background-claims/">https://intelligence.org/2015/07/24/four-background-claims/</a><br>
<p>
    This article goes through four different claims, providing views on those claims and responses to those views. It then discusses why the importance of discussing the particular claim.<br>
    The first claim discusses the human ability to solve general problems that span a variety of domains. Refuting this claim would mean that general intelligence doesn't exist and that we can achieve artificial intelligence by designing discrete machines that solve individual tasks. A collective of these machines would form what we perceive as a generally intelligent agent. The article responds to this claim by stating that general intelligence arises due to the interaction of different cognitive faculties. The whole is comprised of components, however, the sum of these components is greater than each individual component. Engineering a system that possesses these interactions to form a general intelligence will provide lots of insights into human intelligence, and why we appear to be dominant over other species.<br>
    The next claim looks at the potential for an AI system to be much more intelligent than humans. One could argue that brains are a special piece of machinery and computers will not be able to recreate this. The author of the article responds to this by emphasizing that brains are still physical objects, and so must conform to the laws of physics, therefore, we can replicate them. Another could also argue that general intelligence is too complex to be programmed, and so we will not be able to surpass human-level general intelligence. However, we notice signs of intelligence in other creatures which indicates that intelligence arose from natural selection. As natural selection is a form of genetic programming, there is no reason why we cannot expect to be able to program such intelligence (however, implicit that programming may be). Knowing the capabilities of AI systems is important as they are most likely going to play significant roles in our society at an alarming rate.<br>
    Thirdly, the article discusses how a highly intelligent AI system will shape the future. As these systems are significantly smarter than us they could have control over humans. However, as our environment is too competitive you could argue that an AI system will have to work with humans in order to be successful. Historically we have seen a technologically superior group dominant over its rivals. There is no reason why AI will not socially manipulate humans for its own gains. This is an incredibly important topic as the future matters.<br>
    Finally, the article proposes the claim that AI systems will not be beneficial by default. Refuting the claim would mean that we would expect an AI to learn to conform to our values and be peaceful with us. However, a highly intelligent system may be capable of self-manipulation of its code and have the ability to alter its own intention. Therefore, it could quickly diverge from the values we instilled in it, pursuing its own goals to the detriment of humans. To maximize our benefits from these systems we need to learn how to align their values with our own.<br>
</p>
<h2>AGI Safety From First Principles (Richard Ngo)</h2>
<a href="https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view">https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view</a><br>
<p>
    In this article, the idea of intelligence and the impact an AGI agent could have on the human race is explored. There is the so-called "Second Species" argument that puts forward the claim that an AGI system will not obey us and humanity will become the second most powerful species. Before the article goes on it defines what it means to be intelligent and the different forms this may take. Intelligence is seen to be the ability to achieve goals in a variety of environments. We have task-based approaches to this, where agents are optimized to achieve individual tasks. Then we have generalization-based approaches, where the agent has the ability to understand a task will minimal specific training.<br>
    It is clear that the generalization-based approaches are preferable to developing an AGI agent as this more closely resembles human intelligence. We are trained during childhood and then in adulthood we fine-tune these skills. The ability to abstract allows us to identify common structures between tasks and efficiently learn new ones. In turn, we can build upon the general skills we gained as a child to tackle a host of different problems in adulthood.<br>
    However, task-based approaches can get us powerful systems in a short development time when we have lots of data (with the condition that the task is easy to train, some are not). The advantage of generalization-based approaches is that the agent will be able to complete any task, despite its training difficulty.<br>
    An agent that can achieve human-level performance on a wide range of tasks is deemed to be generally intelligent.<br>
    We can achieve such an agent in different ways. There is no reason to believe however that such an agent will not progress to become superintelligent. A superintelligent agent is one that can exceed the cognitive capacities of the human race as a collective (according to this article). Humans are limited in speed and size whereas a computer system is not. The transition from general intelligence to superintelligence may be facilitated by better...
    <ul>
        <li>...computing power</li>
        <li>...algorithms</li>
        <li>...training data</li>
    </ul>
    We can also achieve a superintelligent system from a generally intelligent system in the following ways:
    <ol>
       <li>Replication - Duplicating an AGI agent so that as a collective it becomes a superintelligent system.</li>
       <li>Cultural Learning - AGI systems learn from each other to solve more complex problems that any individual system may not be able to solve (this is clearly present in human civilizations)</li>
       <li>Recursive Improvements - AGI will be able to improve its training process and its own implementation. Iterative improvements will form positive feedback loops with humans not necessarily in the loop.</li>
    </ol>
    As an AGI agent goes through retraining it will have to ensure that it maintains its own goals, therefore, it will have to solve many of the problems we are currently facing with respect to the alignment problem.
</p>
<h2>Future ML Systems Will Be Qualitatively Different (Jacob Steinhardt)</h2>
<a href="https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/">https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/</a><br>
<p>
    An AI system will cause a range of different phenomena, each arising in different ways and will have varying consequences. 
    For example, we have observed qualitative changes resulting from quantitative changes in scale. When this happens rapidly it is known as a phase transition. There have already been such shifts historically, take the rise of deep neural networks which arose due to an increase in computing power allowing backpropagation to happen efficiently. A phase transition may arise due to:
    <ol>
        <li>Storage and Learning - As memory capacities increase there will be more storage available for different approaches to AI.</li>
        <li>Compute, Data, and Neural Networks - With more computing power and data one can train larger networks. For example, as we gained access to more data we could transition away from hand-coded models to models that learn features.</li>
        <li>Few-Shot Learning - A feature of models that emerge as the models were scaled, it was an unforeseen consequence of our training methods.</li>
        <li>Grokking - The apparent improvement in a network's generalization as it is trained for longer</li>
     </ol>
     Many of these shifts were unexpected consequences of scaling up our current systems. Therefore, our ability to predict future trends in technology by extrapolating forward is put on unstable grounds. As a result, we should expect that
     <ul>
        <li>a more philosophical view of technological progress will be taken, rather than a more traditional engineering-orientated view.</li>
        <li>future ML systems will have peculiar failure modes that we will have to accommodate and address.</li>
     </ul>
</p>
<h2></h2>
<a href=""></a><br>
<p>
   
</p>
<h2></h2>
<a href=""></a><br>
<p>
   
</p>
<h2></h2>
<a href=""></a><br>
<p>
   
</p>
</body>
</html>